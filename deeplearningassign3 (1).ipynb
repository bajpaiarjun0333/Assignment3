{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimport random\nfrom datetime import datetime\nimport pandas","metadata":{"execution":{"iopub.status.busy":"2023-05-19T03:47:51.119136Z","iopub.execute_input":"2023-05-19T03:47:51.119511Z","iopub.status.idle":"2023-05-19T03:47:51.125461Z","shell.execute_reply.started":"2023-05-19T03:47:51.119482Z","shell.execute_reply":"2023-05-19T03:47:51.124533Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_input=None\ntrain_output=None\nvalid_input=None\nvalid_output=None\ntest_input=None\ntest_output=None","metadata":{"execution":{"iopub.status.busy":"2023-05-19T03:47:51.272763Z","iopub.execute_input":"2023-05-19T03:47:51.273066Z","iopub.status.idle":"2023-05-19T03:47:51.281419Z","shell.execute_reply.started":"2023-05-19T03:47:51.273039Z","shell.execute_reply":"2023-05-19T03:47:51.280219Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def loadData():\n    global train_input, train_output, valid_input, valid_output, test_input, test_output\n    data_train = pandas.read_csv('/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_train.csv', header=None)\n    train_input = data_train.iloc[:,0]\n    train_output = data_train.iloc[:,1]\n    data_valid = pandas.read_csv('/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_valid.csv', header=None)\n    valid_input = data_valid.iloc[:,0]\n    valid_output = data_valid.iloc[:,1]\n    data_test = pandas.read_csv('/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_test.csv', header=None)\n    test_input = data_test.iloc[:,0]\n    test_output = data_test.iloc[:,1]","metadata":{"execution":{"iopub.status.busy":"2023-05-19T03:47:51.431803Z","iopub.execute_input":"2023-05-19T03:47:51.432080Z","iopub.status.idle":"2023-05-19T03:47:51.440577Z","shell.execute_reply.started":"2023-05-19T03:47:51.432056Z","shell.execute_reply":"2023-05-19T03:47:51.437908Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"SOW_token = 0\nEOW_token = 1\nloadData()","metadata":{"execution":{"iopub.status.busy":"2023-05-19T03:47:51.570405Z","iopub.execute_input":"2023-05-19T03:47:51.570828Z","iopub.status.idle":"2023-05-19T03:47:51.711323Z","shell.execute_reply.started":"2023-05-19T03:47:51.570791Z","shell.execute_reply":"2023-05-19T03:47:51.710142Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class Dictionary:\n    def __init__(self):\n        self.char2index = {}\n        self.index2char = {0: \"#\", 1: \"$\"}\n        self.n_chars = 2  # Count SOS and EOS\n\n    def allWords(self, words):\n        for word in words:\n            for c in word:\n                if c not in self.char2index:\n                    self.char2index[c]=self.n_chars\n                    self.index2char[self.n_chars]=c\n                    self.n_chars+=1\n                \n    def wordToTensor(self,word):\n        a=[]\n        for i in word:\n            a.append(self.char2index[i])\n        a.append(EOW_token)\n        return torch.tensor(a, dtype=torch.long, device=device).view(-1, 1)\n    \n    def createBatches(self, words, batch_size):\n        x=[]\n        for word in words:\n            x.append(self.wordToTensor(word))\n        batches=[]\n        for i in range(0,len(x),batch_size):\n            if batch_size+i >= len(x):\n                break\n            temp=(nn.utils.rnn.pad_sequence(x[i:i+batch_size]).squeeze(2)).to(device)\n            batches.append(temp)\n        return batches\n    \n    def driver(self, words,batch_size):\n        self.allWords(words)\n        return self.createBatches(words,batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-05-19T03:47:51.713653Z","iopub.execute_input":"2023-05-19T03:47:51.714043Z","iopub.status.idle":"2023-05-19T03:47:51.730971Z","shell.execute_reply.started":"2023-05-19T03:47:51.714011Z","shell.execute_reply":"2023-05-19T03:47:51.730112Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class EncoderRNN(nn.Module):\n    def __init__(self, input_size, params):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = params[\"hidden_size\"]\n        self.dropout = nn.Dropout(params[\"dropout_encoder\"])\n        self.num_layers = params[\"num_layers\"]\n        self.batch_size = params[\"batch_size\"]\n        self.embedding_size = params[\"embedding_size\"]\n        self.cell_type = params[\"cell_type\"]\n        self.embedding = nn.Embedding(input_size, params[\"embedding_size\"])\n        self.bidirection = params[\"bidirection\"]\n        if(params[\"cell_type\"] == \"GRU\"):\n            self.gru = nn.GRU(params[\"embedding_size\"], params[\"hidden_size\"], params[\"num_layers\"], dropout = params[\"dropout_encoder\"], bidirectional = params[\"bidirection\"])\n        elif(params[\"cell_type\"] == \"LSTM\"):\n            self.lstm = nn.LSTM(params[\"embedding_size\"], params[\"hidden_size\"], params[\"num_layers\"], dropout = params[\"dropout_encoder\"], bidirectional = params[\"bidirection\"])\n        elif(params[\"cell_type\"] == \"RNN\"):\n            self.rnn = nn.RNN(params[\"embedding_size\"], params[\"hidden_size\"], params[\"num_layers\"], dropout = params[\"dropout_encoder\"], bidirectional = params[\"bidirection\"])\n\n    def forward(self, input, hidden):\n        embedded = self.embedding(input).view(-1,self.batch_size, self.embedding_size)\n        output = self.dropout(embedded)\n        if(self.cell_type == \"GRU\"):\n            _, hidden = self.gru(output, hidden)\n        elif(self.cell_type == \"LSTM\"):\n            _, (hidden, cell) = self.lstm(output)\n#             print(hidden.shape, cell.shape)\n        elif(self.cell_type == \"RNN\"):\n            _, hidden = self.rnn(output, hidden)\n        if self.bidirection:\n            hidden = hidden.reshape(2, hidden.size(0)//2, hidden.size(1), hidden.size(2))\n            hidden = torch.add(hidden[0]*0.5, hidden[1]*0.5)\n            if(self.cell_type == \"LSTM\"):\n                cell = cell.reshape(2, cell.size(0)//2, cell.size(1), cell.size(2))\n                cell = torch.add(cell[0]*0.5, cell[1]*0.5)\n        if self.cell_type == \"LSTM\":\n            return hidden, cell\n        else:\n            return hidden\n\n    def initHidden(self):\n        if self.bidirection:\n            return torch.zeros(2*self.num_layers, self.batch_size, self.hidden_size, device=device)\n        else:\n            return torch.zeros(self.num_layers, self.batch_size, self.hidden_size, device=device)","metadata":{"execution":{"iopub.status.busy":"2023-05-19T03:49:17.197010Z","iopub.execute_input":"2023-05-19T03:49:17.197411Z","iopub.status.idle":"2023-05-19T03:49:17.216362Z","shell.execute_reply.started":"2023-05-19T03:49:17.197340Z","shell.execute_reply":"2023-05-19T03:49:17.215399Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self, params, output_size):\n        super(DecoderRNN, self).__init__()\n        self.hidden_size = params[\"hidden_size\"]\n        self.dropout = nn.Dropout(params[\"dropout_decoder\"])\n        self.num_layers = params[\"num_layers\"]\n        self.batch_size = params[\"batch_size\"]\n        self.embedding_size = params[\"embedding_size\"]\n        self.embedding = nn.Embedding(output_size, params[\"embedding_size\"])\n        self.cell_type = params[\"cell_type\"]\n        if(params[\"cell_type\"] == \"GRU\"):\n            self.gru = nn.GRU(params[\"embedding_size\"], params[\"hidden_size\"], params[\"num_layers\"], dropout = params[\"dropout_decoder\"])\n        elif(params[\"cell_type\"] == \"LSTM\"):\n            self.lstm = nn.LSTM(params[\"embedding_size\"], params[\"hidden_size\"], params[\"num_layers\"], dropout = params[\"dropout_decoder\"])\n        elif(params[\"cell_type\"] == \"RNN\"):\n            self.rnn = nn.RNN(params[\"embedding_size\"], params[\"hidden_size\"], params[\"num_layers\"], dropout = params[\"dropout_decoder\"])\n        self.out = nn.Linear(params[\"hidden_size\"], output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, hidden):\n        output = self.embedding(input).view(-1, self.batch_size, self.embedding_size)\n        output = self.dropout(output)\n        output = torch.relu(output)\n        if(self.cell_type == \"GRU\"):\n            output, hidden = self.gru(output, hidden)\n        elif(self.cell_type == \"LSTM\"):\n            output, (hidden, cell) = self.lstm(output, (hidden[0], hidden[1]))\n        elif(self.cell_type == \"RNN\"):\n            output, hidden = self.rnn(output, hidden)\n        if self.cell_type == \"LSTM\":\n            return self.softmax(self.out(output[0])), hidden, cell\n        return self.softmax(self.out(output[0])), hidden\n\n    def initHidden(self):\n        return torch.zeros(self.num_layers, self.batch_size, self.hidden_size, device=device)","metadata":{"execution":{"iopub.status.busy":"2023-05-19T03:52:34.197704Z","iopub.execute_input":"2023-05-19T03:52:34.198080Z","iopub.status.idle":"2023-05-19T03:52:34.213528Z","shell.execute_reply.started":"2023-05-19T03:52:34.198051Z","shell.execute_reply":"2023-05-19T03:52:34.212304Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class Combine:\n    def __init__(self,encoder, decoder, epochs, learning_rate, batch_size, embedding_size, cell_type, train_batch_input, train_batch_target, valid_batch_input):\n        self.teacher_forcing_ratio = 0.5\n        self.trainIters(encoder, decoder, epochs, learning_rate, batch_size, embedding_size, cell_type, train_batch_input, train_batch_target, valid_batch_input)\n        \n    def findAccuracy(self, input, actual_output, cell_type, n, batch_size):\n        correct = 0\n        for i in range(len(input)):\n            output_word = self.evaluate(encoder1, decoder1, input[i], cell_type, batch_size)\n            for j in range(i*batch_size, i*batch_size+batch_size):\n                if(actual_output[j] == output_word[j-i*batch_size]):\n                    correct += 1\n        return correct/n*100\n    \n\n    def train(self, input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, batch_size, cell_type):\n        encoder_hidden = encoder.initHidden()\n\n        encoder_optimizer.zero_grad()\n        decoder_optimizer.zero_grad()\n\n        input_length = input_tensor.size(0)\n        target_length = target_tensor.size(0)\n\n        loss = 0\n        if cell_type == \"LSTM\":\n            encoder_hidden, encoder_cell = encoder(input_tensor, encoder_hidden)\n        else:\n            encoder_hidden = encoder(input_tensor, encoder_hidden)\n\n        decoder_input = torch.tensor([SOW_token]*batch_size, device=device)\n\n        decoder_hidden = encoder_hidden\n        if cell_type == \"LSTM\":\n            decoder_cell = encoder_cell\n\n        use_teacher_forcing = True if random.random() < self.teacher_forcing_ratio else False\n\n        if use_teacher_forcing:\n            # Teacher forcing: Feed the target as the next input\n            for di in range(target_length):\n                if cell_type == \"LSTM\":\n                    decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, (decoder_hidden, decoder_cell))\n                else:\n                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n                loss += criterion(decoder_output, target_tensor[di])\n                decoder_input = target_tensor[di]  # Teacher forcing\n\n        else:\n            # Without teacher forcing: use its own predictions as the next input\n            for di in range(target_length):\n                if cell_type == \"LSTM\":\n                    decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, (decoder_hidden, decoder_cell))\n                else:\n                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n                topv, topi = decoder_output.topk(1)\n                decoder_input = topi.squeeze().detach()  # detach from history as input\n\n                loss += criterion(decoder_output, target_tensor[di])\n\n        loss.backward()\n        encoder_optimizer.step()\n        decoder_optimizer.step()\n\n        return loss.item() / target_length\n    \n    def trainIters(self, encoder, decoder, epochs, learning_rate, batch_size, embedding_size, cell_type, train_batch_input, train_batch_target, valid_batch_input):\n        plot_losses = []\n        print_loss_total = 0  # Reset every print_every\n        plot_loss_total = 0  # Reset every plot_every\n\n        encoder_optimizer = optim.NAdam(encoder.parameters(), lr=learning_rate, weight_decay = 0.0005)\n        decoder_optimizer = optim.NAdam(decoder.parameters(), lr=learning_rate, weight_decay = 0.0005)\n        criterion = nn.CrossEntropyLoss()\n\n        for epochNum in range(epochs):\n            print(\"Epoch \", epochNum ,\" started \", datetime.now())\n            for i in range(len(train_batch_input)):\n                loss = self.train(train_batch_input[i], train_batch_target[i], encoder,\n                             decoder, encoder_optimizer, decoder_optimizer, criterion, batch_size, cell_type)\n                print_loss_total += loss*batch_size\n\n            print_loss_avg = print_loss_total / len(train_input)\n            print_loss_total = 0\n            print(\"Average loss after \", epochNum+1, \"epochs is \", print_loss_avg)\n    #         train_accuracy = findAccuracy(train_batch_input, train_output, cell_type, len(train_input), batch_size)\n    #         print(\"Train accuracy is \", train_accuracy)\n\n            valid_accuracy = self.findAccuracy(valid_batch_input, valid_output, cell_type, len(valid_input), batch_size)\n            print(\"Valid accuracy is \", valid_accuracy)\n            \n    def evaluate(self, encoder, decoder, input_tensors, cell_type, batch_size):\n        with torch.no_grad():\n\n            input_length = input_tensors.size(0)\n            encoder_hidden = encoder.initHidden()\n\n            if cell_type == \"LSTM\":\n                encoder_hidden, encoder_cell = encoder(input_tensors, encoder_hidden)\n            else:\n                encoder_hidden = encoder(input_tensors, encoder_hidden)\n\n            decoder_input = torch.tensor([SOW_token]*batch_size, device=device)  # SOW\n\n            decoder_hidden = encoder_hidden\n            if cell_type == \"LSTM\":\n                decoder_cell = encoder_cell\n\n            decoded_words = [\"\"]*batch_size\n\n            for di in range(input_length):\n\n                if cell_type == \"LSTM\":\n                    decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, (decoder_hidden, decoder_cell))\n                else:\n                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n                topv, topi = decoder_output.data.topk(1)\n                for i in range(batch_size):\n                    if topi[i].item() == EOW_token or topi[i] == 0:\n                        continue\n                    else:\n                        decoded_words[i] += hin.index2char[topi[i].item()]\n\n                decoder_input = topi.squeeze().detach()\n\n            return decoded_words","metadata":{"execution":{"iopub.status.busy":"2023-05-19T03:57:08.556477Z","iopub.execute_input":"2023-05-19T03:57:08.556864Z","iopub.status.idle":"2023-05-19T03:57:08.584367Z","shell.execute_reply.started":"2023-05-19T03:57:08.556835Z","shell.execute_reply":"2023-05-19T03:57:08.583014Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"params={\"hidden_size\":256,\n        \"num_layers\":2,\n        \"dropout_encoder\":0.2,\n        \"dropout_decoder\":0.2,\n        \"batch_size\":32,\n        \"embedding_size\": 256,\n        \"epochs\":2,\n        \"cell_type\":\"GRU\",\n        \"learning_rate\":0.001,\n        \"bidirection\":True\n        }\n\neng=Dictionary()\nhin=Dictionary()\ntrain_batch_input=eng.driver(train_input,params[\"batch_size\"])\ntrain_batch_target=hin.driver(train_output,params[\"batch_size\"])\nvalid_batch_input=eng.driver(valid_input,params[\"batch_size\"])\n\nencoder1 = EncoderRNN(eng.n_chars, params).to(device)\ndecoder1 = DecoderRNN(params, hin.n_chars).to(device)\n\nCombine(encoder1, decoder1, params[\"epochs\"], params[\"learning_rate\"], params[\"batch_size\"], params[\"embedding_size\"], params[\"cell_type\"], train_batch_input, train_batch_target, valid_batch_input)\n\n#print(\"Train Accuracy started \", datetime.now())\n#train_batch_input = getBatchedTensorFromWords(train_input, batch_size, lang_input)\n#train_accuracy = findAccuracy(train_batch_input, train_output, cell_type, len(train_input), batch_size)\n#print(\"Train accuracy is \", train_accuracy)\n\n#print(\"Test Accuracy started \", datetime.now())\n\n#test_batch_input = getBatchedTensorFromWords(test_input, batch_size, lang_input)\n#test_accuracy = findAccuracy(test_batch_input, test_output, cell_type, len(test_input), batch_size)\n#print(\"Test accuracy is \", test_accuracy)\n#print(\"Test Accuracy ended \", datetime.now())","metadata":{"execution":{"iopub.status.busy":"2023-05-19T03:57:08.740540Z","iopub.execute_input":"2023-05-19T03:57:08.741189Z","iopub.status.idle":"2023-05-19T03:59:03.029616Z","shell.execute_reply.started":"2023-05-19T03:57:08.741148Z","shell.execute_reply":"2023-05-19T03:59:03.028660Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Epoch  0  started  2023-05-19 03:57:14.627565\nAverage loss after  1 epochs is  0.9990693296526014\nValid accuracy is  19.23828125\nEpoch  1  started  2023-05-19 03:58:08.797132\nAverage loss after  2 epochs is  0.6578222709421359\nValid accuracy is  24.21875\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"<__main__.Combine at 0x7a39bf28bd60>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}